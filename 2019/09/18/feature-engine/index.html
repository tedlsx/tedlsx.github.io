<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Feature Engine | 404</title><meta name="description" content="Feature Engine"><meta name="keywords" content="feature engine"><meta name="author" content="shixuan liu"><meta name="copyright" content="shixuan liu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://tedlsx.github.io/2019/09/18/feature-engine/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Feature Engine"><meta name="twitter:description" content="Feature Engine"><meta name="twitter:image" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/06/aq.png"><meta property="og:type" content="article"><meta property="og:title" content="Feature Engine"><meta property="og:url" content="http://tedlsx.github.io/2019/09/18/feature-engine/"><meta property="og:site_name" content="404"><meta property="og:description" content="Feature Engine"><meta property="og:image" content="https://www.analyticsindiamag.com/wp-content/uploads/2019/06/aq.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="Probability Graph" href="http://tedlsx.github.io/2019/09/26/probability-graph/"><link rel="next" title="Kernel" href="http://tedlsx.github.io/2019/09/16/kernel/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#feature"><span class="toc-number">1.</span> <span class="toc-text"> Feature</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#reason-to-do-feature-selection"><span class="toc-number">1.1.</span> <span class="toc-text"> Reason to do feature selection</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-importance"><span class="toc-number">2.</span> <span class="toc-text"> Feature Importance</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#p-value"><span class="toc-number">2.0.1.</span> <span class="toc-text"> P-value</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#f-test"><span class="toc-number">2.0.2.</span> <span class="toc-text"> F-test</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mutual-infomation"><span class="toc-number">2.0.3.</span> <span class="toc-text"> Mutual infomation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#variance-threshold"><span class="toc-number">2.0.4.</span> <span class="toc-text"> Variance threshold</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#some-tips-to-check"><span class="toc-number">3.</span> <span class="toc-text"> Some tips to check</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#power-transformation"><span class="toc-number">4.</span> <span class="toc-text"> Power transformation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#standardization-variance-scaling"><span class="toc-number">4.1.</span> <span class="toc-text"> Standardization (variance scaling)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#l2-normalization"><span class="toc-number">4.2.</span> <span class="toc-text"> l2l^2l2 normalization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-selection"><span class="toc-number">5.</span> <span class="toc-text"> Feature Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#filtering"><span class="toc-number">5.1.</span> <span class="toc-text"> Filtering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#wrapper-methods"><span class="toc-number">5.2.</span> <span class="toc-text"> Wrapper methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#embedded-methods"><span class="toc-number">5.3.</span> <span class="toc-text"> Embedded methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#text-data"><span class="toc-number">6.</span> <span class="toc-text"> Text Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bag-of-words"><span class="toc-number">6.1.</span> <span class="toc-text"> Bag of words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bag-of-n-grams"><span class="toc-number">6.2.</span> <span class="toc-text"> Bag of n Grams</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#filtering-the-text-features"><span class="toc-number">7.</span> <span class="toc-text"> Filtering the Text Features</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#stopwords"><span class="toc-number">7.1.</span> <span class="toc-text"> Stopwords</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stemming词干"><span class="toc-number">7.2.</span> <span class="toc-text"> Stemming(词干)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parsing-and-tokenization"><span class="toc-number">7.3.</span> <span class="toc-text"> Parsing and Tokenization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#collocation-and-phrase"><span class="toc-number">8.</span> <span class="toc-text"> Collocation and Phrase</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#collocation-extraction"><span class="toc-number">8.1.</span> <span class="toc-text"> Collocation Extraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-idf"><span class="toc-number">9.</span> <span class="toc-text"> Tf-Idf</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#using-logistic-regression"><span class="toc-number">10.</span> <span class="toc-text"> Using Logistic Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-hashing"><span class="toc-number">11.</span> <span class="toc-text"> Feature Hashing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bin-counting"><span class="toc-number">12.</span> <span class="toc-text"> Bin Counting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rare-categories"><span class="toc-number">12.1.</span> <span class="toc-text"> Rare Categories</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pca"><span class="toc-number">13.</span> <span class="toc-text"> PCA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-means"><span class="toc-number">14.</span> <span class="toc-text"> K-means</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#image-feature"><span class="toc-number">15.</span> <span class="toc-text"> Image Feature</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#image-gradients"><span class="toc-number">15.1.</span> <span class="toc-text"> Image Gradients</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fully-connected-layers"><span class="toc-number">16.</span> <span class="toc-text"> Fully Connected Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#convolutional-layers"><span class="toc-number">17.</span> <span class="toc-text"> Convolutional Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rectified-linear-unit-relu-transformation"><span class="toc-number">18.</span> <span class="toc-text"> Rectified Linear Unit (ReLU) Transformation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pooling-layers"><span class="toc-number">19.</span> <span class="toc-text"> Pooling Layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#an-item-based-recommender"><span class="toc-number">20.</span> <span class="toc-text"> An Item-Based Recommender</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#import-data-cleaning-and-feature-selection"><span class="toc-number">20.1.</span> <span class="toc-text"> Import Data, Cleaning and Feature Selection</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#binning"><span class="toc-number">21.</span> <span class="toc-text"> Binning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">22.</span> <span class="toc-text"> Reference</span></a></li></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://www.analyticsindiamag.com/wp-content/uploads/2019/06/aq.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">404</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="https://live.staticflickr.com/65535/49241182478_46e7d63095_t.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> Gallery</span></a><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Feature Engine</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-09-18<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-03-09</time><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4.4k</span><span class="post-meta__separator">|</span><span>Reading time: 27 min</span><span class="post-meta__separator">|</span><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p>Introduction of feature engineering. Include some methods in Pyhton.</p>
<h2 id="feature"><a class="markdownIt-Anchor" href="#feature"></a> Feature</h2>
<h3 id="reason-to-do-feature-selection"><a class="markdownIt-Anchor" href="#reason-to-do-feature-selection"></a> Reason to do feature selection</h3>
<ol>
<li>Because a large number of features may cost long training time</li>
<li>Increasing number of features may increase the risk of overfitting<br>
It can also help us to reduce the dimension of our dataset without loss main information.</li>
</ol>
<h2 id="feature-importance"><a class="markdownIt-Anchor" href="#feature-importance"></a> Feature Importance</h2>
<p>Which provide a important score for each feature. And usually can be done for the raw data</p>
<h4 id="p-value"><a class="markdownIt-Anchor" href="#p-value"></a> P-value</h4>
<p>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="comment">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class="line">x = pd.DataFrame() <span class="comment"># feature df</span></span><br><span class="line">y = pd.DataFrame() <span class="comment"># target df</span></span><br><span class="line">x_1 = sm.add_constant(x) <span class="comment"># add a constant column to df x</span></span><br><span class="line"><span class="comment">#Fitting sm.OLS model</span></span><br><span class="line">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class="line"><span class="comment"># Which prints out the p-value of each features in this model as an array </span></span><br><span class="line">model.pvalues</span><br></pre></td></tr></table></figure>
<p>Write a function to select feature based on p-value:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x is feature df</span></span><br><span class="line"><span class="comment"># y is target df</span></span><br><span class="line"><span class="comment"># sl is significant level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backwardElimination</span><span class="params">(x, y, sl)</span>:</span></span><br><span class="line">    cols = list(x.columns)</span><br><span class="line">    pmax = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> (len(cols) &gt; <span class="number">0</span>):</span><br><span class="line">        p= []                                                                                   </span><br><span class="line">        X_1 = x[cols]</span><br><span class="line">        X_1 = sm.add_constant(X_1)</span><br><span class="line">        model = sm.OLS(y, X_1).fit()</span><br><span class="line">        p = pd.Series(model.pvalues.values[<span class="number">1</span>:], index = cols)      </span><br><span class="line">        pmax = max(p)</span><br><span class="line">        <span class="comment"># .idxmax returns the index of maximum</span></span><br><span class="line">        feature_with_p_max = p.idxmax()</span><br><span class="line">        <span class="keyword">if</span>(pmax &gt; sl):</span><br><span class="line">            cols.remove(feature_with_p_max)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    selected_features_BE = cols</span><br><span class="line">    print(selected_features_BE)</span><br><span class="line"></span><br><span class="line">backwardElimination(x, y, <span class="number">0.05</span>)</span><br></pre></td></tr></table></figure>
<h4 id="f-test"><a class="markdownIt-Anchor" href="#f-test"></a> F-test</h4>
<p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p>
<p>Here introduced the <a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">skitlearn package</a>, we will use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" target="_blank" rel="noopener">F-test</a> to find first K best features:</p>
<p>For continues data type</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> sklearn.feature_selection</span><br><span class="line">sklearn.feature_selection.f_regression(x, y) <span class="comment"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class="line"><span class="comment"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure>
<p>For classification data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.f_classif(x, y) <span class="comment"># same with f_regression</span></span><br><span class="line">sklearn.feature_selection.chi2(x, y) <span class="comment"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure>
<p>F-score is good for linear relation</p>
<h4 id="mutual-infomation"><a class="markdownIt-Anchor" href="#mutual-infomation"></a> Mutual infomation</h4>
<p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>
More detail in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" target="_blank" rel="noopener">sklearn Mutual information</a><br>
Which is good for non-linear relation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x is feature df, y is target df</span></span><br><span class="line"><span class="comment"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class="line"><span class="comment"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class="line">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class="number">3</span>, copy=<span class="literal">True</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure>
<h4 id="variance-threshold"><a class="markdownIt-Anchor" href="#variance-threshold"></a> Variance threshold</h4>
<p>Which is only care about feature itself: if it is not vary a lot, then it has poor predictive power.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>
<h2 id="some-tips-to-check"><a class="markdownIt-Anchor" href="#some-tips-to-check"></a> Some tips to check</h2>
<p>Firstly, we can check whether the data is magnitude matter or we just need to know whether it is positive/negative.</p>
<p>For the data span several orders of magnitude or output depends on the scale of the input, such as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mi>X</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2X-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>, k-means clustering, nearest neighbors methods (KNN), radial basis function (RBF) kernel and other methods which used Euclidean distance, it is often a good idea to normalize the features to let the output stays at a expected scale.</p>
<p>For the logistic function, which is not sensitive to the scale. For example decision tree, gradient boosted machines and random forests. But sometime the input scale may increase which will let the features grows out of the range that tree was trained on. Then we may need to rescale the input or do the bin-counting method.</p>
<p>It is also very important to check the distribution for the numeric features. For the linear regression, we assume the prediction error follows the Gaussian distribution. In the case that prediction target spread over several order of magnitude which may reject this assumption. Then we may need to use Log Transformation (power transform)</p>
<h2 id="power-transformation"><a class="markdownIt-Anchor" href="#power-transformation"></a> Power transformation</h2>
<p>Instead of Log transformation, A simple generalization of both the square root transform and log-transform is Box-Cox transform:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><msup><mi>x</mi><mi>λ</mi></msup><mo>−</mo><mn>1</mn></mrow><mi>λ</mi></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if </mtext><mi>λ</mi><mi mathvariant="normal">≠</mi><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>l</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mtext>if </mtext><mi>λ</mi><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\hat{x} =
  \begin{cases}
    \frac{x^{\lambda}-1}{\lambda}       &amp; \quad \text{if } \lambda \neq 0\\
    ln(x)  &amp; \quad \text{if } \lambda = 0
  \end{cases}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7074600000000002em;"><span style="top:-3.7074599999999998em;"><span class="pstrut" style="height:3.0429199999999996em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0429199999999998em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">λ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9270285714285713em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">λ</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.26746em;"><span class="pstrut" style="height:3.0429199999999996em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2074599999999998em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7074600000000002em;"><span style="top:-3.7074599999999998em;"><span class="pstrut" style="height:3.0429199999999996em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if </span></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.26746em;"><span class="pstrut" style="height:3.0429199999999996em;"></span><span class="mord"><span class="mspace" style="margin-right:1em;"></span><span class="mord text"><span class="mord">if </span></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2074599999999998em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>To use Box-Cox transformation, we need the data be positive. If it is not, we can add a fixed positive constant. When applying this method, we also need to get the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>. The way to get <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> are maximum likelihood (find the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> which maximizes the Gaussian likelihood  or Bayesian methods). The package <strong>Scipy</strong> include these:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">df_log = stats_boxcox(df, <span class="keyword">lambda</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">df_boxcox , boxcox_params = stats.boxcox(df)</span><br></pre></td></tr></table></figure>
<p>##Feature Scaling</p>
<p>If the model is sensitive to the scale of the input, then we need to scale the features</p>
<p>###Min-Max Scaling</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat{x} = \frac{x-min(x)}{max(x)-min(x)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h3 id="standardization-variance-scaling"><a class="markdownIt-Anchor" href="#standardization-variance-scaling"></a> Standardization (variance scaling)</h3>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><msqrt><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\hat{x}= \frac{x-mean(x)}{\sqrt{var(x)}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.557em;vertical-align:-1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.175em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-2.8950000000000005em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30499999999999994em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.13em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>Note that, if we use these 2 methods to the sparse features, the sparse value may become dense which dramatically increase the computation</p>
<h3 id="l2-normalization"><a class="markdownIt-Anchor" href="#l2-normalization"></a> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>l</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">l^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> normalization</h3>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mover accent="true"><mi>x</mi><mo>^</mo></mover></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mi>x</mi><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mn>2</mn></msub></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mn>2</mn></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msubsup><mi>x</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\hat{x} &amp;=\frac{x}{||x||_2} \\
 ||x||_2 &amp;=  \sqrt{x_1^2+x_2^2+...+x_n^2}
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.48356em;vertical-align:-1.9917799999999994em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.4917800000000003em;"><span style="top:-4.682895em;"><span class="pstrut" style="height:3.2986750000000002em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">^</span></span></span></span></span></span></span></span><span style="top:-2.1482200000000007em;"><span class="pstrut" style="height:3.2986750000000002em;"></span><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9917799999999994em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.4917800000000003em;"><span style="top:-4.682895em;"><span class="pstrut" style="height:3.2986750000000002em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.1482200000000007em;"><span class="pstrut" style="height:3.2986750000000002em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2986750000000002em;"><span class="svg-align" style="top:-3.8em;"><span class="pstrut" style="height:3.8em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959080000000001em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.0448000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959080000000001em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.0448000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.26630799999999993em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.740108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.258675em;"><span class="pstrut" style="height:3.8em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.8800000000000001em;"><svg width="400em" height="1.8800000000000001em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"><path d="M1001,80H400000v40H1013.1s-83.4,268,-264.1,840c-180.7,
572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,
-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744c-10,12,-21,25,-33,39s-32,39,-32,39
c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30c26.7,-32.7,52,-63,76,-91s52,-60,52,-60
s208,722,208,722c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,
-658.5c53.7,-170.3,84.5,-266.8,92.5,-289.5c4,-6.7,10,-10,18,-10z
M1001 80H400000v40H1013z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5413249999999998em;"><span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9917799999999994em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>##Interaction Features</p>
<p>An easy way to extend the linear model to include the interaction features is to add the product of each 2 features. But is cost a lot computation which <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span> goes to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>There are some methods to deal with it: we can only select the top information of these interaction features. Or handcraft some complex features.</p>
<h2 id="feature-selection"><a class="markdownIt-Anchor" href="#feature-selection"></a> Feature Selection</h2>
<p>Feature selection is a method that try to select the useful features in order to reduce the complexity of the model.</p>
<h3 id="filtering"><a class="markdownIt-Anchor" href="#filtering"></a> Filtering</h3>
<p>We can compute the p-value for each features: the correlation or the mutual information between each features and the response variable. The disadvantage is this method can not consider the model which means the selected features may not suitable for the model</p>
<h3 id="wrapper-methods"><a class="markdownIt-Anchor" href="#wrapper-methods"></a> Wrapper methods</h3>
<p>This method is try to provide the score for each subset of features in this model. The advantage is it will not delete the uninformative features but the combination of them is useful. The disadvantage is it cost a lot of computation</p>
<h3 id="embedded-methods"><a class="markdownIt-Anchor" href="#embedded-methods"></a> Embedded methods</h3>
<p>This method is the part of the traing process, for example: the decision tree need to find the feature to split the tree. Or the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">l_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> regularizer is trying to add a sparsity constraint to the model. Hence this method is specific to the model.</p>
<h2 id="text-data"><a class="markdownIt-Anchor" href="#text-data"></a> Text Data</h2>
<h3 id="bag-of-words"><a class="markdownIt-Anchor" href="#bag-of-words"></a> Bag of words</h3>
<p>Bag of words (BoW) which is a list of word vector and the counts of these words. Each word becomes a feature here.</p>
<p>The problem can be it will not show the correct information of the text. For example, “not bad” means “good” but in BoW these 2 words are separate.</p>
<h3 id="bag-of-n-grams"><a class="markdownIt-Anchor" href="#bag-of-n-grams"></a> Bag of n Grams</h3>
<p>This is a extension of BoW, it used a window to select the n words from begin to end and create a count vector.</p>
<p>For example: I like cute cat. If we still use BoW, it will be 1-gram or called <strong>unigram</strong>. The feature will be “I”, “like”, “cute”, “cat”. The 2-grams or called <strong>“bigrams”</strong> will select features: “I like”, “like cute”, “cute cat”.</p>
<p>Bag of n grams usually become more sparse and larger which leads to more computation and store. The larger <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> means more information and more cost.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="comment"># Load the first 10000 words in "data" json file</span></span><br><span class="line">f = open(<span class="string">'data.json'</span>) </span><br><span class="line">js=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">	js.append(json.loads(f.readline()))</span><br><span class="line"></span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(js)</span><br><span class="line"><span class="comment"># Create feature transformers for unigrams, bigrams, and trigrams.</span></span><br><span class="line"><span class="comment"># The default ignores single-character words, which is useful in practice because # it trims uninformative words, but we explicitly include them in this example for # illustration purposes.</span></span><br><span class="line">bow_converter = CountVectorizer(token_pattern=<span class="string">'(?u)\\b\\w+\\b'</span>)</span><br><span class="line">bigram_converter = CountVectorizer(ngram_range=(<span class="number">2</span>,<span class="number">2</span>), token_pattern=<span class="string">'(?u)\\b\\w+\\b'</span>)</span><br><span class="line">trigram_converter = CountVectorizer(ngram_range=(<span class="number">3</span>,<span class="number">3</span>), token_pattern=<span class="string">'(?u)\\b\\w+\\b'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the transformers and look at vocabulary size &gt;&gt;&gt; bow_converter.fit(review_df['text'])</span></span><br><span class="line">words = bow_converter.get_feature_names()</span><br><span class="line">bigram_converter.fit(review_df[<span class="string">'text'</span>])</span><br><span class="line">bigrams = bigram_converter.get_feature_names() </span><br><span class="line">trigram_converter.fit(review_df[<span class="string">'text'</span>])</span><br><span class="line">trigrams = trigram_converter.get_feature_names()</span><br><span class="line"><span class="keyword">print</span> (len(words), len(bigrams), len(trigrams)) </span><br><span class="line"><span class="comment"># 26047 346301 847545</span></span><br></pre></td></tr></table></figure>
<h2 id="filtering-the-text-features"><a class="markdownIt-Anchor" href="#filtering-the-text-features"></a> Filtering the Text Features</h2>
<p>Since we know in text data, there are capitalized word, grammer, phrase and stop words. Then we need to introduce several method to filter</p>
<h3 id="stopwords"><a class="markdownIt-Anchor" href="#stopwords"></a> Stopwords</h3>
<p>Word such like “on”, “and”, “a” do not affect the meaning of the sentence but do take a position in features.  The NLP package in Python, such like NLTK include a list of stop words for many languages.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nltk.download()</span><br></pre></td></tr></table></figure>
<p>In stop word list, it is good to combine “didn” and “‘t’” together. Otherwise, it will be count as “didn” and “t” as 2 words.</p>
<h3 id="stemming词干"><a class="markdownIt-Anchor" href="#stemming词干"></a> Stemming(词干)</h3>
<p>Word such like “dog” and “dogs” or “run”, “ran” and “running” may count as  different words but has same meaning when we do some analysis. Stemming method is a NLP task to map thses different words as the same   one. This method may only focus on English language which means it is not a common method for all language.  For example in NLTK</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">stemmer = nltk.stem.porter.PorterStemmer()</span><br><span class="line"><span class="comment">## try running, ran</span></span><br><span class="line">stemmer.stem(<span class="string">"running"</span>)</span><br><span class="line"><span class="comment"># run</span></span><br><span class="line">stemmer.stem(<span class="string">"ran"</span>)</span><br><span class="line"><span class="comment"># run</span></span><br></pre></td></tr></table></figure>
<p>But somtimes the unique word such like “new” and “news” has different meaning. This is the disadvantage of this method.</p>
<h3 id="parsing-and-tokenization"><a class="markdownIt-Anchor" href="#parsing-and-tokenization"></a> Parsing and Tokenization</h3>
<p>Parsing is a method which also consider the structure or the form of the original text. For example, if the original text is a Email, we need to pay more attention on header. Without parsing, the words in header will be treated as normal words which we may lose some useful information.</p>
<p>Tokenization transforms the string to a sequence of characters then into a sequence of tokens. Each token treated as a word. Tokenization need to know which character is end for a token and the end for the next token. For example, space characters or the “#” delimiters can be good separators.</p>
<p>Working only with sentence or the paragraph ranthe than whole document, <strong>word2vec</strong> is a better method.</p>
<p>Note that most strings encoded as <strong>ASCII</strong> or <strong>Unicode</strong>.</p>
<h2 id="collocation-and-phrase"><a class="markdownIt-Anchor" href="#collocation-and-phrase"></a> Collocation and Phrase</h2>
<p>Tokens represent the list of the words or n-grams which is not good as Phrase. The Phrase or called <strong>collocation</strong> which is a combination of 2 or more words that is usually used in convention way.</p>
<p>Collocation has more meaning than the separate words, such like: “strong tea” where “strong” is no longer the physical strength here. Moreover, the “cute cat” has the same meaning for the separate and the sum of these 2 words. Then we do not consider this as a collocation. Also, the collocation not required the the words stay together. Such like “play with the cat” contain collocation “play cat”.</p>
<h3 id="collocation-extraction"><a class="markdownIt-Anchor" href="#collocation-extraction"></a> Collocation Extraction</h3>
<p>One idea is trying to do the hypothesis test where</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">H_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>: word A appears independent from word B <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⇒</mo><mi>p</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Rightarrow p(B|A) = p(B|not A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">t</span><span class="mord mathdefault">A</span><span class="mclose">)</span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">H_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>: word A change the likelihood of seeing word B  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⇒</mo><mi>p</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mi mathvariant="normal">≠</mi><mi>p</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Rightarrow p(B|A) \neq p(B|not A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel"><span class="mrel"><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="rlap"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="inner"><span class="mrel"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">n</span><span class="mord mathdefault">o</span><span class="mord mathdefault">t</span><span class="mord mathdefault">A</span><span class="mclose">)</span></span></span></span></p>
<p>The statistic is the log-likelihood ratio:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>λ</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">;</mo><msub><mi>H</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">;</mo><msub><mi>H</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">log\lambda = log \frac{L(Data; H_0)}{L(Data; H_1)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>There is an assumption of the data: Let the words generation follows the binomial distribution. Where for each word, we toss a coin, if it is head then we place this word otherwise we insert some other word.</p>
<p>The process can be:</p>
<ul>
<li>Find the probability of appearance for each single word based on frequency of this word</li>
<li>Find the conditional probability of 2 words for all unique bigrams <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(A|B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span></li>
<li>Compute the likelihood ratio <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>λ</mi></mrow><annotation encoding="application/x-tex">log\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">λ</span></span></span></span> for all unique bigrams</li>
<li>Sort the likelihood ratio and pick the small value of these bigrams as features</li>
</ul>
<h2 id="tf-idf"><a class="markdownIt-Anchor" href="#tf-idf"></a> Tf-Idf</h2>
<p>Some common word like “is”, “the” appear many times but have less meaning. However, “dramatic”, “magnificently” help us to understand the information in sentence which means they are valuable. Next, we will discuss how to find them out.</p>
<p>Tf-Idf stands for term-frequency-inverse-document-frequency. Rather than count each word in each documents in dataset, Tf-Idf looks at the normalized count where each count of word is divided by the number of the documents this word appears.</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>b</mi><mi>o</mi><mi>w</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>frenquency of the word w appears in document d</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>t</mi><mi>f</mi><mi mathvariant="normal">_</mi><mi>i</mi><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>bow(w,d)*N/(number of documents that word w appears)</mtext></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
bow(w,d) &amp;=  \text{frenquency of the word w appears in document d} \\
tf\_idf(w,d) &amp;= \text{bow(w,d)*N/(number of documents that word w appears)}
\end{aligned}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0000000000000004em;vertical-align:-1.2500000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord">frenquency of the word w appears in document d</span></span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord text"><span class="mord">bow(w,d)*N/(number of documents that word w appears)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>Where bow stands for Bag-of-Word and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> is the total number of documents in the dataset. And the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>/(number of documents that word w appears) is called inverse document frequency. For this frequency, we know that if the word <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> appears in many documents, then this frequency will close to 1. If this word <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> appears only in few documents, then this frequency will become much higher.</p>
<p>Note that we can also take a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">log</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span> on this frequency which is just a alternative way to show this frequency. The word appears in many documents will lead this frequency close to 0 and appears in few documents will make this frequency much bigger than before.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bow_transform = text.CountVectorizer()</span><br><span class="line">X_train_bow = bow_transform.fit_transform(training_data[<span class="string">"text"</span>])</span><br><span class="line">X_test_bow = bow_transform.transform(test_data[<span class="string">"text"</span>])</span><br><span class="line"><span class="comment">### show the length of the bag of words</span></span><br><span class="line">len(bow_transform.vocabulary_)</span><br><span class="line"></span><br><span class="line">y_training = training_data[<span class="string">"target"</span>]</span><br><span class="line">y_test = training_data[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">## Then the Tf-Idf is</span></span><br><span class="line">tfidf_train_transform = text.TfidfTransformer(norm=<span class="literal">None</span>)</span><br><span class="line">X_train_tfidf = tfidf_train_transformer.fit_transform(X_train_bow)</span><br><span class="line">X_test_tfidf = tfidf_train_transformer.transform(X_test_bow)</span><br><span class="line"></span><br><span class="line"><span class="comment">## And the l_2 normalize of bag of word</span></span><br><span class="line">X_train_l2 = preproc.normalize(X_train_bow, axis = <span class="number">0</span>)</span><br><span class="line">X_test_l2 = preproc.normalize(X_test_bow, axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="using-logistic-regression"><a class="markdownIt-Anchor" href="#using-logistic-regression"></a> Using Logistic Regression</h2>
<p>Here we used Logistic Regression to build the model and compared the result for bag-of-word, tf-idf and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> method.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">(X_train, y_train, X_test, y_test, description)</span>:</span></span><br><span class="line">  model = LogisticRegression().fit(X_train, y_train)</span><br><span class="line">  score = model.score(X_test, y_test)</span><br><span class="line">  print(<span class="string">"The test has score with"</span>, description, <span class="string">"features:"</span>, score)</span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>Using this function we can compare these three method and the highest score means this method is better. But we need to know the Logistic regression is not a good classifier. Especially when the number of features are greater the number of observations, we need to use <strong>regularization</strong> to solve the high dimensional problem.</p>
<h2 id="feature-hashing"><a class="markdownIt-Anchor" href="#feature-hashing"></a> Feature Hashing</h2>
<p>Hash function can map a potentially unbounded integer to a finite integer range <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1, m]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">m</span><span class="mclose">]</span></span></span></span>. Since many numbers may map into a same bin, then it is also callled collision. A uniform hash function assured that almost same number of numbers are mapped into each <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">m</span></span></span></span> bins.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hash_features</span><span class="params">(word_list, m)</span>:</span></span><br><span class="line">  output = [<span class="number">0</span>] * m</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">    index = hash_fun(word) % m</span><br><span class="line">    output[index] += <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>Or use the package in scikit-learn and check the size of them. Moreover, hash mapping has a built-in data structure in Python called dictionary: dic={“key”: value, “key2”: value}.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">h = FeatureHasher(n_feature=m, input_type=<span class="string">"string"</span>)</span><br><span class="line">f = h.transform(dataframe)</span><br><span class="line"></span><br><span class="line"><span class="comment">## check the storage size</span></span><br><span class="line"><span class="keyword">from</span> sys <span class="keyword">import</span> getsizeof</span><br><span class="line">print(<span class="string">"Pandas Series"</span>, getsizeof(dataframe))</span><br><span class="line">print(<span class="string">"Hashed numpy array"</span>, getsizeof(f))</span><br></pre></td></tr></table></figure>
<h2 id="bin-counting"><a class="markdownIt-Anchor" href="#bin-counting"></a> Bin Counting</h2>
<p>Bin Counting selects the features for individual category variables by finding the conditional probability of the target under that value of the category rather than using the value of the category.</p>
<p>Example in “Big Learning Made Easy-with Counts” is</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| user | number of clicks | number of nonclicks | probability of clicks |</span><br><span class="line">|------|------------------|---------------------|-----------------------|</span><br><span class="line">| A    | 5                | 120                 | 0.04                  |</span><br><span class="line">| B    | 20               | 230                 | 0.08                  |</span><br></pre></td></tr></table></figure>
<p>Hence here we used probability of clicks to replace the 2 original features. Others, such like odds ratio and log-odds ratio can replace the probability method.</p>
<p>The advantage of this method is to make a large, sparse, binary representation of the categorical variable into a small size by using statisitcal value to replace the original data.</p>
<h3 id="rare-categories"><a class="markdownIt-Anchor" href="#rare-categories"></a> Rare Categories</h3>
<p>One way to deal with it called back-off, creat a bin to sum the rare category data. For the definition of the rare categories, we use a threshod to compute them.</p>
<h2 id="pca"><a class="markdownIt-Anchor" href="#pca"></a> PCA</h2>
<p>PCA stands for principal componnet analysis, which is good when data lies in a linear subspace.</p>
<h2 id="k-means"><a class="markdownIt-Anchor" href="#k-means"></a> K-means</h2>
<p>When data has a more complicated shape, k-means is useful here. Also other method such like LR, kNN, Random Forest and RBF SVM can do the same thing but have different efficiency.</p>
<h2 id="image-feature"><a class="markdownIt-Anchor" href="#image-feature"></a> Image Feature</h2>
<h3 id="image-gradients"><a class="markdownIt-Anchor" href="#image-gradients"></a> Image Gradients</h3>
<p><em>Image gradient</em> is the differences between neighboring pixels(像素). But the problem is that individual pixels do not carry enough semantic information about the image. Therefore, they are bad atomic units for analysis.</p>
<p>Two simple way is calculating the difference horizontal (x) and vertical (y) axes of the image. The mask <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">–</mi><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1, 0, –1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord" style="margin-right:0.02778em;">–</span><span class="mord">1</span><span class="mclose">]</span></span></span></span> takes the difference between the left neighbor and the right neighbor or the up-neighbor and the down-neighbor. We used the <strong>convolution</strong> which is common in signal processing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data, color</span><br><span class="line"><span class="comment">### Load the example image and turn it into grayscale </span></span><br><span class="line">image = color.rgb2gray(data.chelsea())</span><br><span class="line"><span class="comment">### Compute the horizontal gradient using the centered 1D filter.</span></span><br><span class="line"><span class="comment">### This is equivalent to replacing each non-border pixel with the</span></span><br><span class="line"><span class="comment">### difference between its right and left neighbors. The leftmost</span></span><br><span class="line"><span class="comment">### and rightmost edges have a gradient of 0.</span></span><br><span class="line">gx = np.empty(image.shape, dtype=np.double) </span><br><span class="line">gx[:,<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">gx[:, <span class="number">-1</span>] = <span class="number">0</span></span><br><span class="line">gx[:, <span class="number">1</span>:<span class="number">-1</span>] = image[:, :<span class="number">-2</span>] - image[:, <span class="number">2</span>:]</span><br><span class="line"><span class="comment">### Same deal for the vertical gradient</span></span><br><span class="line">gy = np.empty(image.shape, dtype=np.double) </span><br><span class="line">gy[<span class="number">0</span>,:]=<span class="number">0</span></span><br><span class="line">gy[<span class="number">-1</span>, :] = <span class="number">0</span></span><br><span class="line">gy[<span class="number">1</span>:<span class="number">-1</span>, :] = image[:<span class="number">-2</span>, :] - image[<span class="number">2</span>:, :]</span><br><span class="line"><span class="comment">### Matplotlib incantations</span></span><br><span class="line">fig, (ax1, ax2, ax3) = plt.subplots(<span class="number">3</span>, <span class="number">1</span>,figsize=(<span class="number">5</span>, <span class="number">9</span>),sharex=<span class="literal">True</span>,sharey=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.axis(<span class="string">'off'</span>)</span><br><span class="line">ax1.imshow(image, cmap=plt.cm.gray) </span><br><span class="line">ax1.set_title(<span class="string">'Original image'</span>) </span><br><span class="line">ax1.set_adjustable(<span class="string">'box'</span>)</span><br><span class="line">ax2.axis(<span class="string">'off'</span>)</span><br><span class="line">ax2.imshow(gx, cmap=plt.cm.gray)</span><br><span class="line">ax2.set_title(<span class="string">'Horizontal gradients'</span>) </span><br><span class="line">ax2.set_adjustable(<span class="string">'box'</span>)</span><br><span class="line">ax3.axis(<span class="string">'off'</span>)</span><br><span class="line">ax3.imshow(gy, cmap=plt.cm.gray) </span><br><span class="line">ax3.set_title(<span class="string">'Vertical gradients'</span>) </span><br><span class="line">ax3.set_adjustable(<span class="string">'box'</span>)</span><br></pre></td></tr></table></figure>
<p>Image feature extractors such as SIFT and HOG  are better.</p>
<p>In 1999, computer vision researchers figured out a better way to represent images using statistics of image patches: the <em>Scale Invariant Feature Transform</em> (SIFT) [Lowe, 1999].</p>
<p>SIFT was originally developed for the task of object recognition, which involves not only correctly tagging the image as containing an object, but pinpointing its location in the image. The process involves analyzing the image at a pyramid of possible scales, detecting interest points that could indicate the presence of the object, extract‐ ing features (commonly called <em>image descriptors</em> in computer vision) about the inter‐ est points, and determining the pose of the object.</p>
<p>Over the years, the usage of SIFT expanded to extract features not only for interest points but across the entire image. The SIFT feature extraction procedure is very sim‐ ilar to another technique, called the Histogram of Oriented Gradients (HOG) [Dalal and Triggs, 2005]. Both of them essentially compute histograms of gradient orienta‐ tions. We now describe this process in detail.</p>
<h2 id="fully-connected-layers"><a class="markdownIt-Anchor" href="#fully-connected-layers"></a> Fully Connected Layers</h2>
<p>A fully connected neural network is simply a set of linear functions of all of the input features. Recall that a linear function can be written as an inner product between the input feature vector and a weight vector, plus a possible constant term. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y=Wx+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span></span></span></span>. The reason why it called fully connected is every input can be  used in output and there is no restriction on the Weighted matrix <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>. (We know the convolutional layer only use a subset of imputes for each output)</p>
<h2 id="convolutional-layers"><a class="markdownIt-Anchor" href="#convolutional-layers"></a> Convolutional Layers</h2>
<p>Convolutional Layers use only a subset of the inputs for each output. The transformation or called convolution operator moves across the input.</p>
<p><em>Applying a simple Gaussian filter on an image</em>. Example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data, color</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># First create X,Y meshgrids of size 5x5 on which we compute the Gaussian</span></span><br><span class="line">ind = [<span class="number">-1.</span>, <span class="number">-0.5</span>, <span class="number">0.</span>, <span class="number">0.5</span>, <span class="number">1.</span>]</span><br><span class="line">X,Y = np.meshgrid(ind, ind)</span><br><span class="line">X</span><br><span class="line"><span class="comment">## array([[-1. , -0.5,  0. ,  0.5,  1. ],</span></span><br><span class="line"> <span class="comment">##    [-1. , -0.5,  0. ,  0.5,  1. ],</span></span><br><span class="line">  <span class="comment">#    [-1. , -0.5,  0. ,  0.5,  1. ],</span></span><br><span class="line">  <span class="comment">#    [-1. , -0.5,  0. ,  0.5,  1. ],</span></span><br><span class="line">  <span class="comment">#    [-1. , -0.5,  0. ,  0.5,  1. ]])</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># G is a simple, unnormalized Gaussian kernel where the value at (0,0) is 1.0</span></span><br><span class="line">G = np.exp(-(np.multiply(X,X) + np.multiply(Y,Y))/<span class="number">2</span>)</span><br><span class="line">G</span><br><span class="line"><span class="comment">## result of G</span></span><br><span class="line">array([[<span class="number">0.36787944</span>, <span class="number">0.53526143</span>, <span class="number">0.60653066</span>, <span class="number">0.53526143</span>, <span class="number">0.36787944</span>],</span><br><span class="line">       [<span class="number">0.53526143</span>, <span class="number">0.77880078</span>, <span class="number">0.8824969</span> , <span class="number">0.77880078</span>, <span class="number">0.53526143</span>],</span><br><span class="line">       [<span class="number">0.60653066</span>, <span class="number">0.8824969</span> , <span class="number">1.</span>        , <span class="number">0.8824969</span> , <span class="number">0.60653066</span>],</span><br><span class="line">       [<span class="number">0.53526143</span>, <span class="number">0.77880078</span>, <span class="number">0.8824969</span> , <span class="number">0.77880078</span>, <span class="number">0.53526143</span>],</span><br><span class="line">       [<span class="number">0.36787944</span>, <span class="number">0.53526143</span>, <span class="number">0.60653066</span>, <span class="number">0.53526143</span>, <span class="number">0.36787944</span>]])</span><br><span class="line"></span><br><span class="line">cat = color.rgb2gray(data.chelsea())</span><br><span class="line">blurred_cat = signal.convolve2d(cat, G, mode=<span class="string">'valid'</span>)</span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">4</span>), sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>)</span><br><span class="line">ax1.axis(<span class="string">'off'</span>)</span><br><span class="line">ax1.imshow(cat, cmap=plt.cm.gray)</span><br><span class="line">ax1.set_title(<span class="string">'Input image'</span>)</span><br><span class="line">ax1.set_adjustable(<span class="string">'box'</span>)</span><br><span class="line">ax2.axis(<span class="string">'off'</span>)</span><br><span class="line">ax2.imshow(blurred_cat, cmap=plt.cm.gray)</span><br><span class="line">ax2.set_title(<span class="string">'After convolving with a Gaussian filter'</span>)</span><br><span class="line">ax2.set_adjustable(<span class="string">'box'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## To read our own data img</span></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">ws_image = imageio.imread(<span class="string">"wushuang.png"</span>)</span><br><span class="line">ws_image = color.rgb2gray(ws_image)</span><br><span class="line">ws_image = np.loadtxt(<span class="string">"wushuang.txt"</span>)</span><br><span class="line"><span class="comment"># where ws_image is ndarray of RGB</span></span><br></pre></td></tr></table></figure>
<h2 id="rectified-linear-unit-relu-transformation"><a class="markdownIt-Anchor" href="#rectified-linear-unit-relu-transformation"></a> Rectified Linear Unit (ReLU) Transformation</h2>
<p>Since we know the <strong>activation function</strong> between the output and the input usually be a non-linear transformation such like <strong>tanh</strong> function (a smooth nonlinear function bounded between -1 and 1), the <strong>sigmoid</strong> function (a smooth function non-linear function between 0 and 1) or called the <strong>rectified linear unit</strong>. ReLU is a linear function where the negative part is zeroed out and the range of it is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0, \infty)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∞</span><span class="mclose">)</span></span></span></span></p>
<p>Common Activation Function:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLU(x) = max(0, x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x) = \frac{sinh(x)}{cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">sigmoid(x) = \frac{1}{1+e^{-x}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">o</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><img alt="activation_function" data-src="https://live.staticflickr.com/65535/49638539661_b4dc1e3ff8_z.jpg" class="lozad"></p>
<h2 id="pooling-layers"><a class="markdownIt-Anchor" href="#pooling-layers"></a> Pooling Layers</h2>
<p>Pooling Layer combine every neighborhood under its length to produce the output. This reduce the number of outputs in the hidden layer of deep learning network, which is effectively reduces the probability of overfitting the network to training data.</p>
<p>The methods to pool the inputs are: averaging, summing and the maximum value. AlexNet uses overlapping max pooling, moving through the image in strides of two pixels (or outputs) and pooling through three neighbors.</p>
<p><img alt="max pooling" data-src="https://live.staticflickr.com/65535/49638815477_7202f34fbb_z.jpg" class="lozad"></p>
<h2 id="an-item-based-recommender"><a class="markdownIt-Anchor" href="#an-item-based-recommender"></a> An Item-Based Recommender</h2>
<ol>
<li>Generalize information about the item</li>
<li>Score all other items and find the similar ones.</li>
<li>Return the score and the items</li>
</ol>
<h3 id="import-data-cleaning-and-feature-selection"><a class="markdownIt-Anchor" href="#import-data-cleaning-and-feature-selection"></a> Import Data, Cleaning and Feature Selection</h3>
<p>First, we need to define the what knid of papers are more useful for the users. The featues such like: Published date, Fields of study can be assumed to be useful. We may calculate  the similarity score to define each papers. Usually, the <strong>cosine</strong> similarity provides a reasonable comparison between 2 non-zero vectors. (other similarity can be viewed in <a href="%22https://tedlsx.github.io/2019/08/30/distance/%22">distance</a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_json(<span class="string">"filepath/filename.json"</span>, lines = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">## df.shape. df.columns</span></span><br><span class="line"></span><br><span class="line">df = df[feature == <span class="string">"value"</span>].drop_duplicates(feature = <span class="string">"value"</span>, keep = <span class="string">"first"</span>).drop([<span class="string">"feature_1"</span>, <span class="string">"feature_2"</span>], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## df.shape</span></span><br><span class="line"></span><br><span class="line">unique_fos = sorted(list(&#123;feature</span><br><span class="line">													<span class="keyword">for</span> paper_row <span class="keyword">in</span> model_df.fos.fillna(<span class="string">'0'</span>)</span><br><span class="line">													<span class="keyword">for</span> feature <span class="keyword">in</span> paper_row &#125;))</span><br><span class="line">unique_year = sorted(model_df[<span class="string">'year'</span>].astype(<span class="string">'str'</span>).unique())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_array</span><span class="params">(x, var, unique_array)</span>:</span></span><br><span class="line">		row_dict = &#123;&#125; </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x.index: </span><br><span class="line">      var_dict = &#123;&#125;</span><br><span class="line">			<span class="keyword">for</span> j <span class="keyword">in</span> range(len(unique_array)): </span><br><span class="line">        <span class="keyword">if</span> type(x[i]) <span class="keyword">is</span> list:</span><br><span class="line">						<span class="keyword">if</span> unique_array[j] <span class="keyword">in</span> x[i]:</span><br><span class="line">								var_dict.update(&#123;var + <span class="string">'_'</span> + unique_array[j]: <span class="number">1</span>&#125;)</span><br><span class="line">						<span class="keyword">else</span>:</span><br><span class="line">								var_dict.update(&#123;var + <span class="string">'_'</span> + unique_array[j]: <span class="number">0</span>&#125;)</span><br><span class="line">				<span class="keyword">else</span>:</span><br><span class="line">						<span class="keyword">if</span> unique_array[j] == str(x[i]):</span><br><span class="line">								var_dict.update(&#123;var + <span class="string">'_'</span> + unique_array[j]: <span class="number">1</span>&#125;) </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                var_dict.update(&#123;var + <span class="string">'_'</span> + unique_array[j]: <span class="number">0</span>&#125;)</span><br><span class="line">    	row_dict.update(&#123;i : var_dict&#125;)</span><br><span class="line">		feature_df = pd.DataFrame.from_dict(row_dict, dtype=<span class="string">'str'</span>).T </span><br><span class="line">    <span class="keyword">return</span> feature_df</span><br><span class="line"><span class="comment">###</span></span><br><span class="line">year_features = feature_array(model_df[<span class="string">'year'</span>], unique_year) </span><br><span class="line">fos_features = feature_array(model_df[<span class="string">'fos'</span>], unique_fos)</span><br><span class="line">first_features = fos_features.join(year_features).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sys <span class="keyword">import</span> getsizeof</span><br><span class="line">	print(<span class="string">'Size of first feature array: '</span>, getsizeof(first_features))</span><br><span class="line"></span><br><span class="line">  <span class="comment">## define a “good” recommenda‐ tion as a paper that looks similar to the input.</span></span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cosine </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">item_collab_filter</span><span class="params">(features_df)</span>:</span></span><br><span class="line">		item_similarities = pd.DataFrame(index = features_df.columns,</span><br><span class="line">                                 			columns = features_df.columns)</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> features_df.columns: </span><br><span class="line">      	<span class="keyword">for</span> j <span class="keyword">in</span> features_df.columns:</span><br><span class="line">        		item_similarities.loc[i][j] = <span class="number">1</span> - cosine(features_df[i],</span><br><span class="line">                                                 features_df[j])</span><br><span class="line"><span class="keyword">return</span> item_similarities</span><br><span class="line"></span><br><span class="line">first_items = item_collab_filter(first_features.loc[:, <span class="number">0</span>:<span class="number">1000</span>])</span><br><span class="line"></span><br><span class="line"> <span class="comment">## heatmap to show the similarity</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line">sns.set()</span><br><span class="line">ax = sns.heatmap(first_items.fillna(<span class="number">0</span>), vmin=<span class="number">0</span>, vmax=<span class="number">1</span>, cmap=<span class="string">"YlGnBu"</span>, xticklabels=<span class="number">250</span>, yticklabels=<span class="number">250</span>) </span><br><span class="line"></span><br><span class="line">ax.tick_params(labelsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<h2 id="binning"><a class="markdownIt-Anchor" href="#binning"></a> Binning</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bins = int(round((model_df[<span class="string">'year'</span>].max() - model_df[<span class="string">'year'</span>].min()) / <span class="number">10</span>))</span><br><span class="line">temp_df = pd.DataFrame(index = model_df.index)</span><br><span class="line">temp_df[<span class="string">'yearBinned'</span>] = pd.cut(model_df[<span class="string">'year'</span>].tolist(), bins, precision = <span class="number">0</span>)</span><br><span class="line">X_yrs = pd.get_dummies(temp_df[<span class="string">'yearBinned'</span>])</span><br><span class="line">X_yrs.columns.categories</span><br><span class="line">IntervalIndex([(<span class="number">1831.0</span>, <span class="number">1841.0</span>], (<span class="number">1841.0</span>, <span class="number">1851.0</span>], (<span class="number">1851.0</span>, <span class="number">1860.0</span>],</span><br><span class="line">               (<span class="number">1860.0</span>, <span class="number">1870.0</span>], (<span class="number">1870.0</span>, <span class="number">1880.0</span>] ... (<span class="number">1968.0</span>, <span class="number">1978.0</span>],</span><br><span class="line">               (<span class="number">1978.0</span>, <span class="number">1988.0</span>], (<span class="number">1988.0</span>, <span class="number">1997.0</span>], (<span class="number">1997.0</span>, <span class="number">2007.0</span>],</span><br><span class="line">               (<span class="number">2007.0</span>, <span class="number">2017.0</span>]]</span><br><span class="line">              closed=<span class="string">'right'</span>,</span><br><span class="line">              dtype=<span class="string">'interval[float64]'</span>)</span><br><span class="line"><span class="comment"># plot the new distribution</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">X_yrs.sum().plot.bar(ax = ax)</span><br><span class="line">ax.tick_params(labelsize=<span class="number">8</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Binned Years'</span>, fontsize=<span class="number">12</span>) </span><br><span class="line">ax.set_ylabel(<span class="string">'Counts'</span>, fontsize=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<p>Feature Engineering for Machine Learning, 2018, O’Reilly Media</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">shixuan liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://tedlsx.github.io/2019/09/18/feature-engine/">http://tedlsx.github.io/2019/09/18/feature-engine/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/feature-engine/">feature engine    </a></div><div class="post_share"><div class="social-share" data-image="https://www.analyticsindiamag.com/wp-content/uploads/2019/06/aq.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpeg"><div class="post-qr-code__desc">Wechat</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpeg"><div class="post-qr-code__desc">Alipay</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/09/26/probability-graph/"><img class="prev_cover lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>Probability Graph</span></div></a></div><div class="next-post pull-right"><a href="/2019/09/16/kernel/"><img class="next_cover lozad" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Kernel</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '',
  clientSecret: '',
  repo: 'gitalk',
  owner: 'tedlsx',
  admin: 'tedlsx',
  id: md5(decodeURI(location.pathname)),
  language: ''
})
gitalk.render('gitalk-container')</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By shixuan liu</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#feature"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> Feature</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#reason-to-do-feature-selection"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text"> Reason to do feature selection</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#feature-importance"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> Feature Importance</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#p-value"><span class="toc_mobile_items-number">2.0.1.</span> <span class="toc_mobile_items-text"> P-value</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#f-test"><span class="toc_mobile_items-number">2.0.2.</span> <span class="toc_mobile_items-text"> F-test</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#mutual-infomation"><span class="toc_mobile_items-number">2.0.3.</span> <span class="toc_mobile_items-text"> Mutual infomation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#variance-threshold"><span class="toc_mobile_items-number">2.0.4.</span> <span class="toc_mobile_items-text"> Variance threshold</span></a></li></ol></li></ol><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#some-tips-to-check"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> Some tips to check</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#power-transformation"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> Power transformation</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#standardization-variance-scaling"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text"> Standardization (variance scaling)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#l2-normalization"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text"> l2l^2l2 normalization</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#feature-selection"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text"> Feature Selection</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#filtering"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text"> Filtering</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#wrapper-methods"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text"> Wrapper methods</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#embedded-methods"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text"> Embedded methods</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#text-data"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text"> Text Data</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#bag-of-words"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text"> Bag of words</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#bag-of-n-grams"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text"> Bag of n Grams</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#filtering-the-text-features"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text"> Filtering the Text Features</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#stopwords"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text"> Stopwords</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#stemming词干"><span class="toc_mobile_items-number">7.2.</span> <span class="toc_mobile_items-text"> Stemming(词干)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#parsing-and-tokenization"><span class="toc_mobile_items-number">7.3.</span> <span class="toc_mobile_items-text"> Parsing and Tokenization</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#collocation-and-phrase"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text"> Collocation and Phrase</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#collocation-extraction"><span class="toc_mobile_items-number">8.1.</span> <span class="toc_mobile_items-text"> Collocation Extraction</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#tf-idf"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text"> Tf-Idf</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#using-logistic-regression"><span class="toc_mobile_items-number">10.</span> <span class="toc_mobile_items-text"> Using Logistic Regression</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#feature-hashing"><span class="toc_mobile_items-number">11.</span> <span class="toc_mobile_items-text"> Feature Hashing</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#bin-counting"><span class="toc_mobile_items-number">12.</span> <span class="toc_mobile_items-text"> Bin Counting</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#rare-categories"><span class="toc_mobile_items-number">12.1.</span> <span class="toc_mobile_items-text"> Rare Categories</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#pca"><span class="toc_mobile_items-number">13.</span> <span class="toc_mobile_items-text"> PCA</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#k-means"><span class="toc_mobile_items-number">14.</span> <span class="toc_mobile_items-text"> K-means</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#image-feature"><span class="toc_mobile_items-number">15.</span> <span class="toc_mobile_items-text"> Image Feature</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#image-gradients"><span class="toc_mobile_items-number">15.1.</span> <span class="toc_mobile_items-text"> Image Gradients</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#fully-connected-layers"><span class="toc_mobile_items-number">16.</span> <span class="toc_mobile_items-text"> Fully Connected Layers</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#convolutional-layers"><span class="toc_mobile_items-number">17.</span> <span class="toc_mobile_items-text"> Convolutional Layers</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#rectified-linear-unit-relu-transformation"><span class="toc_mobile_items-number">18.</span> <span class="toc_mobile_items-text"> Rectified Linear Unit (ReLU) Transformation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#pooling-layers"><span class="toc_mobile_items-number">19.</span> <span class="toc_mobile_items-text"> Pooling Layers</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#an-item-based-recommender"><span class="toc_mobile_items-number">20.</span> <span class="toc_mobile_items-text"> An Item-Based Recommender</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#import-data-cleaning-and-feature-selection"><span class="toc_mobile_items-number">20.1.</span> <span class="toc_mobile_items-text"> Import Data, Cleaning and Feature Selection</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#binning"><span class="toc_mobile_items-number">21.</span> <span class="toc_mobile_items-text"> Binning</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#reference"><span class="toc_mobile_items-number">22.</span> <span class="toc_mobile_items-text"> Reference</span></a></li></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" data-click="false"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/unitychan.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":40,"vOffset":-10},"mobile":{"show":true},"log":false,"tagMode":false});</script></body></html>