<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>clustering</title>
      <link href="/2019/08/26/clustering/"/>
      <url>/2019/08/26/clustering/</url>
      
        <content type="html"><![CDATA[<p>Clustering is a method to divide data points into a number of groups which the data in this group should have more similarity than others.</p><p>For example: if we know the petal length and sepal length of some flowers, we can divide these flowers into several group.</p><p><img alt="flower" data-src="https://qph.fs.quoracdn.net/main-qimg-afe1eeaa43a9a3ed33e241fb54591be7" class="lozad"></p><h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h2><ol><li>Specify the number of clusters k</li><li>Randomly assign each data to a cluster</li><li>Computer the centroids for each cluster</li><li>Re-assign the points to the closest centroids</li><li>Re-compute the centroids for each cluster</li><li>Repeat 4 and 5 until no improvements</li></ol><h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><p>This can be divided into 2 types: Top-down and Bottom-up</p><p>For Bottom-up: </p><ol><li>From bottom, assign each data as a cluster</li><li>Two closest cluster are merged into one cluster (calculate the distance)</li><li>The best choice of the number of clusters is to find number of vertical line at maximum distance between the mergence . (Here is the number of blue line between A to B)</li></ol><p><img alt="bottom-up" data-src="https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering-7.png" class="lozad"></p><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><ul><li><p>Hierarchical Clustering can not handle the big data but k-means can. k-means has $O(n)$ and HC has $O(n^2)$</p></li><li><p>k-mean is good when cluster has shape circle in 2D, sphere in 3D.</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some Prooffff</title>
      <link href="/2019/08/14/proof/"/>
      <url>/2019/08/14/proof/</url>
      
        <content type="html"><![CDATA[<ul><li>Binomial distribution</li></ul><h2 id="Binomial-distribution"><a href="#Binomial-distribution" class="headerlink" title="Binomial distribution"></a>Binomial distribution</h2><p>$$<br>p(x)=C_k^nn^p(n-k)^{(1-p)}=\frac{n!}{k!(n-k)!}p^k(1-p)^{(n-k)}<br>$$</p><p>$$<br>\begin{aligned}<br>E(x)&amp;=\sum_{k=1}^{n}k\frac{n!}{k!(n-k)!}p^k(1-p)^{(n-k)} \<br>&amp;= \sum_{k=1}^{n}k\frac{n(n-1)!}{k(k-1)!(n-k)!}p p^{k-1}(1-p)^{(n-k)} \<br>&amp;= np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{(n-k)}<br>\end{aligned}<br>$$</p><p>Let $a = k-1$, $b=n-1$ $\Rightarrow k=a+1, n=b+1$<br>$$<br>\begin{aligned}<br>E(x)&amp;= np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{(n-k)} \<br>&amp;= np\sum_{a=0}^{b}\frac{b!}{a!(b-n)!}p^{a}(1-p)^{(b-a)}<br>\end{aligned}<br>$$<br>Where $\sum_{a=0}^{b}\frac{b!}{a!(b-n)!}p^{a}(1-p)^{(b-a)}$ is the sum of all the probabilities which equals to 1.</p><p>Hence the $E(x)$ is $np$ </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Generalized Linear Model</title>
      <link href="/2019/08/08/glm/"/>
      <url>/2019/08/08/glm/</url>
      
        <content type="html"><![CDATA[<p>Objective:</p><ul><li>General linear model (GLM) used to predict response variables with both continuous or discrete distribution and response variables can have linear or non-linear relationship with explanatory variables. </li></ul><p>Model structure: </p><ul><li>GLM includes three parts. Random component, Systematic component, Link function.</li></ul><p>Assumption:  </p><ul><li>$y_i$ is independent and typically assumed to follow an <a href="https://tedlsx.github.io/2019/08/06/exp=family/">exponential family</a> distribution with $\mu_i$. </li><li>GLM assumes transformed dependent variables (through link function) and independent variables has a linear relationship. </li><li>Error $\epsilon_i$ should be independent but not normally distributed.</li></ul><p>Parameter estimate: </p><ul><li>$x_i$ as covariates and $\beta$ for coefficients</li></ul><p>Model selection: </p><ul><li>feature selection </li></ul><p>Model fit: </p><ul><li>Least square <a id="more"></a></li></ul><h2 id="Generalized-Linear-Models-GLM"><a href="#Generalized-Linear-Models-GLM" class="headerlink" title="Generalized Linear Models (GLM)"></a>Generalized Linear Models (GLM)</h2><table><thead><tr><th>Model</th><th>Random</th><th>Link</th><th>Systematic</th></tr></thead><tbody><tr><td>linear regression</td><td>normal</td><td>identity</td><td>continuous</td></tr><tr><td>ANOVA</td><td>normal</td><td>identity</td><td>ategorical</td></tr><tr><td>logistic regression</td><td>binomial</td><td>logit</td><td>mixed</td></tr><tr><td>loglinear</td><td>poisson</td><td>log</td><td>categorical</td></tr><tr><td>poisson regression</td><td>poisson</td><td>log</td><td>mixed</td></tr><tr><td>multinomial response</td><td>multinomial</td><td>generalized logit</td><td>mixed</td></tr></tbody></table><p>Introduction of 3 components of any GLM:<br>random component: which is the probability distribution of response variable Y</p><p>systematic component: show the explanatory variables $(x_1,x_2,..,x_k)$ in the model. And GLM has a strong assumption that $\eta=\beta x$, which means the natural parateter $\eta$ in exponential family should equal to linear predictor (design choice). </p><p>link function: $\eta$ or $g(u)$ - refer to the link between random and systematic components. In another words, it shows how response variables change based on explanatory variables. It can be a non-linear function. In addition, the inverse function $g^{-1}(\eta)=\mu$ is the response function which can reflect linear predictor to the target $y$.</p><p>Then we look some examples for GLM component:</p><h3 id="simple-linear-regression"><a href="#simple-linear-regression" class="headerlink" title="simple linear regression"></a>simple linear regression</h3><p>$$y_I = \beta_0 + \beta x_i + \epsilon_i$$</p><ul><li>Random component: Y is dependent variable and has $y\sim N(\eta,\sigma_e^2)$, $\epsilon_i \sim N(0,\sigma^2)$</li><li>Systematic component:  independent variable X typically be continuous and linear predictor is $\eta = \beta x_i$ </li><li>Link function: identity link , $\eta=g(E(y_i))=E(y_i)$, this is the simple link function which model the mean directly. Hypothesis $h(x) = E(y|x,\beta)=\mu=g^{-1}(\eta)=\mu$</li></ul><h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><p>$$logit(\pi)=log(\frac{\pi}{1-\pi})=\beta_0+\beta x_i$$<br>which is the log odds of probability of “success” as a function of explanatory variables.</p><ul><li>Random component: The distribution of Y is assumed as $Bernoulli(\pi)$, $\pi$ is probability of “success”</li><li>Systematic component: X is independent variables typically be discrete, linear predictor is $\eta = \beta x_i$  </li><li>link function: Logit function, $\eta = logit(\pi)=log(\frac{\pi}{1-\pi})$ which models the log odds of the mean($\mu$). Hypothesis $h(x_i)=E(y_i|\beta,x_i) = sigmoid(\eta_i)$</li></ul><h3 id="Log-linear-model"><a href="#Log-linear-model" class="headerlink" title="Log-linear model"></a>Log-linear model</h3><p>$$log(\mu_{ij}) = \lambda + \lambda_i^A + \lambda_j^B + \lambda_{ij}^{AB}$$</p><ul><li>Random component: The distribution of counts, follows Poisson distribution</li><li>Systematic component: Independent variable X is discrete and is linear in parameters $\lambda + \lambda_i^{x_i}+…+ \lambda_n^{x_n}$</li><li>Link function: Log link, $\eta = log(\mu)$ which model the log of mean.</li></ul><h2 id="Advantage-of-GLM-over-OLS-ordinary-least-square"><a href="#Advantage-of-GLM-over-OLS-ordinary-least-square" class="headerlink" title="Advantage of GLM over OLS(ordinary least square)"></a>Advantage of GLM over OLS(ordinary least square)</h2><ul><li>There is no need to transform the respose $Y$ to have normal distribution.</li><li>Variance no need to be constant</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The exponential family</title>
      <link href="/2019/08/06/exp-family/"/>
      <url>/2019/08/06/exp-family/</url>
      
        <content type="html"><![CDATA[<p>Introduction of exponential family</p><a id="more"></a><h1 id="The-exponential-family"><a href="#The-exponential-family" class="headerlink" title="The exponential family"></a>The exponential family</h1><p>$$<br>p(y;\eta) = b(y) exp(\eta^T T(y) - \alpha(\eta))<br>$$</p><p>where</p><ul><li>$\eta$ is natural parameter</li><li>$T(y)$ is sufficient statistic</li><li>$\alpha(\eta)$ is log partition function</li></ul><p>If $T(y) = y$, then we want to know $E(y|x)$. Hence the expected outcome $y=E(y|x)=h(x)$ </p><h2 id="Gaussian-distribution"><a href="#Gaussian-distribution" class="headerlink" title="Gaussian distribution"></a>Gaussian distribution</h2><p>$$<br>\begin{aligned}<br>p(y,\mu)&amp;=\frac{1}{\sqrt{2\pi}}exp(-\frac{(y-\mu)^2}{2}) \<br>&amp;= \frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2}y^2+y\mu -\frac{1}{2}\mu ^2) \<br>&amp;= \frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2}y^2)exp(y\mu-\frac{1}{2}\mu ^2)<br>\end{aligned}<br>$$</p><p>Hence<br>$$<br>\begin{aligned}<br>b(y)&amp;=\frac{1}{\sqrt{2\pi}} exp(-\frac{1}{2}y^2) \<br>\eta&amp;=\mu \<br>T(\mu) &amp;= y \<br>\alpha &amp;= \frac{1}{2}\mu^2<br>\end{aligned}<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Random Forest</title>
      <link href="/2019/08/06/random-forest/"/>
      <url>/2019/08/06/random-forest/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction-of-Random-Forest"><a href="#Introduction-of-Random-Forest" class="headerlink" title="Introduction of Random Forest"></a>Introduction of Random Forest</h1><p>RF(random forest) is one of the classification method. And it consists of many random decision trees. The “random”  in RF means it used random sample of original data and also used random subset features to build the model.    </p><p>It has several advantage:</p><ul><li>RF includes 2 randomness which makes model not too overfitted and good to avoid noise</li><li>In high dimensional problems, we do not need to do the feature selection and dimension reduction.</li><li>Can also give the importance to each classification</li><li>RF can both works on continuous and categorical data</li><li>It generates an internal unbiased estimate of the generalization error as the forest building progresses</li></ul><h1 id="Introduction-of-Decision-Tree"><a href="#Introduction-of-Decision-Tree" class="headerlink" title="Introduction of Decision Tree"></a>Introduction of Decision Tree</h1><p>DT(decisiona tree) is a tree-like graph and decision model which produce the events outcome, resource costs and utility. DT is a supervised learning methods and are adaptable at solving any kind of problem (classification or regression).</p><p>When split the node into sub-nodes, DT should decide which classification should be top. Here we need use Gini importance: for each split at variable m, the child nodes should has smaller gini impurity criterion than the parent node.</p><h2 id="DT-algorithm"><a href="#DT-algorithm" class="headerlink" title="DT algorithm"></a>DT algorithm</h2><p>The DT algorithms refered as CART(classification and regression trees).</p><p>In <a href="https://scikit-learn.org/stable/modules/tree.html" target="_blank" rel="noopener">skit-learn</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">iris = load_iris()</span><br><span class="line">clf = tree.DecisionTreeClassifier()</span><br><span class="line">clf = clf.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line">tree.plot_tree(clf.fit(iris.data, iris.target))</span><br></pre></td></tr></table></figure><h1 id="How-Random-Forests-work"><a href="#How-Random-Forests-work" class="headerlink" title="How Random Forests work"></a>How Random Forests work</h1><p>It used <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#inter" target="_blank" rel="noopener">out-of-bag(oob)</a> error to get the unbiased estimate and find the importance of variables.</p><p>Also use Gini to find the variables when split from top to down.</p><h2 id="Bagging-and-Boosting"><a href="#Bagging-and-Boosting" class="headerlink" title="Bagging and Boosting"></a>Bagging and Boosting</h2><p>RF belongs to <em>Ensemble Learning</em> in Machine Learning where Ensemble Learning has 2 major algorithms as Bagging and Boosting</p><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><ul><li><p>Using Bootstrapping randomly take n training samples(random draw with replacment). Repeating k times which we can get k training sets. (all k sets are independent)</p></li><li><p>For k trainign sets, we get k models</p></li><li><p>For classification problem, use voting. For regression problems, use average of the results.</p></li></ul><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><ul><li>123123</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Some distributions</title>
      <link href="/2019/08/06/distribution/"/>
      <url>/2019/08/06/distribution/</url>
      
        <content type="html"><![CDATA[<p>Introduction of some distribution</p><a id="more"></a><p>random variable</p><p>probability density function</p><h2 id="Binomial-distribution"><a href="#Binomial-distribution" class="headerlink" title="Binomial distribution"></a>Binomial distribution</h2><p>For example, flip a coin for 5 times, define random variable X as number of heads after 5 flips. The probability is defined as:<br>$$<br>p(X=n)=\frac{5!}{n!(5-n)!}=C_5^n<br>$$<br>The general probability is<br>$$<br>p(X=k)=\frac{n!}{k!(n-k)!}=C_n^k<br>$$<br>And the<br>$$<br>E(X) = np=\sum_{k=0}^{n}kC_n^kp^k(1-p)^{n-k}<br>$$<br>The term for $k=0$ is 0, hence it can be rewrite as<br>$$<br>E(X) =\sum_{k=1}^{n}kC_n^kp^k(1-p)^{n-k}<br>$$<br>Then<br>$$<br>\begin{aligned}<br>E(x) &amp;= \sum_{k=1}^{n}k\frac{n!}{k!(n-k)!}p^k(1-p)^{(n-k)} \<br>&amp;= \sum_{k=1}^{n}k\frac{n(n-1)!}{k(k-1)!(n-k)!}p p^{k-1}(1-p)^{(n-k)} \<br>&amp;= np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{(n-k)}<br>\end{aligned}<br>$$<br>Let $a = k-1$, $b=n-1$ $\Rightarrow k=a+1, n=b+1$<br>$$<br>\begin{aligned}<br>E(x)&amp;= np\sum_{k=1}^{n}\frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{(n-k)} \<br>&amp;= np\sum_{a=0}^{b}\frac{b!}{a!(b-n)!}p^{a}(1-p)^{(b-a)}<br>\end{aligned}<br>$$<br>Where $\sum_{a=0}^{b}\frac{b!}{a!(b-n)!}p^{a}(1-p)^{(b-a)}$ is the sum of all the probabilities which equals to 1.</p><p>Hence the $E(x)$ is $np$ </p><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><p>For example, we want to know X = how many car pass in an hour. </p><p>Say $E(X) = \lambda = np$,  where $\lambda$ has unit cars/hour $\Rightarrow  60min/hour \frac{\lambda}{60} cars/min$ </p><p>Hence the probability can be written as whether there is a car pass in that minute which follows binomial distribution<br>$$<br>p(X=k)=C_{60}^k(\frac{\lambda}{60})^k(1-\frac{\lambda}{60})^{60-k}<br>$$<br>But the question is if there are more than 1 car pass in a minute, how do we deal with it? The most straight way is to set minutes into second<br>$$<br>p(X=k) =C_{3600}^k(\frac{\lambda}{3600})^k(1-\frac{\lambda}{3600})^{3600-k}<br>$$<br>As we know<br>$$<br>\lim_{x\rightarrow \inf}(1+\frac{a}{x})^x=e^a<br>$$<br>The proof is let $1/n = a/x, x= na$ . Then rewrite as:<br>$$<br>\begin{aligned}<br>&amp; \lim_{n\rightarrow \infty}(1+\frac{1}{n})^{na} \<br>&amp;=\lim_{n\rightarrow \infty}((1+\frac{1}{n})^{n})^a \<br>&amp;= e^a<br>\end{aligned}<br>$$<br>Then as we increased $n$ . The probability can be written as<br>$$<br>\begin{aligned}<br>p(X=k) &amp;=\lim_{n\rightarrow \infty} C_{n}^k(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k} \<br>&amp;= \lim_{n\to \infty} \frac{n!}{(n-k)!k!} \frac{\lambda ^k}{n^k}  (1-\frac{\lambda}{n})^{n} (1-\frac{\lambda}{n})^{-k} \<br>&amp;= \lim_{n \to \infty} \frac{n(n-1)…1}{(n-k)(n-k-1)…1}\frac{\lambda ^k}{k!} \lim_{n \to \infty} (1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k} \<br>&amp;= \lim_{n \to \infty} \frac{n^k…}{(n^k…)}\frac{\lambda ^k}{k!} \lim_{n \to \infty} (1-\frac{\lambda}{n})^n(1-\frac{\lambda}{n})^{-k} \<br>&amp;= 1\cdot\frac{\lambda ^k}{k!} e^{-\lambda}\cdot1 \<br>\end{aligned}<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression Model</title>
      <link href="/2019/08/05/linear-model/"/>
      <url>/2019/08/05/linear-model/</url>
      
        <content type="html"><![CDATA[<p>Objective: </p><ul><li>Trying to predict the continuous variable Y which is a linear function of several continus variables x.</li></ul><p>Model structure: </p><ul><li>$$Y_i=\beta_0 + \beta_1 x_i + \epsilon_i$$</li></ul><p>Assumption: </p><ul><li>Y follows normal distribution, error $\epsilon_i$ is indepdent and has $\epsilon_i \sim N(0,\sigma^2)$. Data X is fixed </li></ul><p>Parameter estimate: </p><ul><li>$\beta_0$ as intercept and $\beta_1$ as slope</li></ul><p>Model selection: </p><ul><li>feature selection </li></ul><p>Model fit: </p><ul><li>$R^2$</li><li>residual analysis</li><li>F-statistic<a id="more"></a></li></ul><h2 id="Multiple-Linear-Regression-Model"><a href="#Multiple-Linear-Regression-Model" class="headerlink" title="Multiple Linear Regression Model"></a>Multiple Linear Regression Model</h2><p>Multiple linear regression is a linear model with more than 1 variable. These variables are called dependent variables and the predict variable is called independent variables.</p><p>Where the formula for Multiple linear regression model is:</p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_nx_{ip}+\epsilon_n" class="lozad"></p> for i = 1,...,n is the number of observations or data. $p$ is the number of dependent variables. <p>Hence the matrix notation for multiple linear regression is:</p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_n\end{bmatrix}=\begin{bmatrix}    1 & x_{11} & x_{12} & \dots  & x_{1n} \\    1 & x_{21} & x_{22} & \dots  & x_{2n} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    1 & x_{d1} & x_{d2} & \dots  & x_{dn}\end{bmatrix}\begin{bmatrix}    \beta_0 \\    \beta_1 \\    \vdots \\    \beta_n\end{bmatrix}+\begin{bmatrix}    \epsilon_1 \\    \epsilon_2 \\    \vdots \\    \epsilon_n\end{bmatrix}" class="lozad"></p> <p>Which can also write in simple statement:<br> </p><p align="center"><br><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; Y=X \beta+\epsilon" class="lozad"></p><p></p> <p>Using leat square, we need to minimise this function<br> </p><p align="center"><br><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \sum_{i=1}^{n}\epsilopn_i^2=\epsilon^T\epsilon=(y-X\beta)^T(y-X\beta)" class="lozad"></p><p></p><p>To find the ‘best’  $\beta$. One way is to find the relation between $y-\hat{y}$ and $X$:</p> <p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \hat{Y}=X \hat{\beta}" class="lozad"></p> <p><img alt="A test image" data-src="linear-model/orth.png" class="lozad"></p><p>In this figure, the residuals $y-\hat{y}$ are <a href="http://mathworld.wolfram.com/Orthogonal.html" target="_blank" rel="noopener">orthogonal</a> to the columns of $X$:</p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; X^T(y-X\hat{\beta})=0" class="lozad"></p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \Leftrightarrow X^Ty-X^TX\hat{\beta}=0" class="lozad"></p> <p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \Leftrightarrow X^TX\hat{\beta}=X^Ty" class="lozad"></p> <p>Then we can define </p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \hat{\beta}=(X^TX)^{-1}X^Ty" class="lozad"></p> <p>Also we can use <strong>Least Squares</strong> as our loss(error) function which can minimize the Eucledian distance between the predicted $\hat{y}$ and actual $y$:</p><p>$Loss function =L = \frac{1}{2}\sum_{i=1}^{n}(y_i-\beta^Tx_i)^2=\frac{1}{2}||y-\beta x||^2 = \frac{1}{2}(y-x\beta)^T(y-x\beta)$</p><p>Finding the minium of the loss function, we can use differentiate for $\beta$:</p><p>$$\frac{dL}{d\beta}=-X^Ty+X^TX\beta=0$$</p><p>We still got the result:</p><p>$$\hat{\beta}=(X^TX)^{-1}X^Ty$$</p><p>This of course works only if the inverse exists. If the inverse does not exist, the normal equations can still be solved, but the solution may not be unique.</p><p>For fitted $\hat{y}$, we can plug in the $\hat{\beta}$</p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \hat{y}=X\hat{\beta}=X(X^TX)^{-1}X^Ty=Hy" class="lozad"></p> <p>The matrix $H$ (Hat-matrix) is a $n*n$ matrix, it maps the observed values $y$ onto the fitted value $\hat{y}$</p><p>And residuals can be written as </p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; \epsilon=y-\hat{y}=y-X\hat{\beta}=y-Hy=(I-H)y" class="lozad"></p> <p>Here is a comparison between my own code build with <a href="https://numpy.org" target="_blank" rel="noopener">numpy</a> performance and the package in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.htmln" target="_blank" rel="noopener">sckit-learn</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="comment"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class="line">y = np.dot(X, np.array([<span class="number">1</span>, <span class="number">2</span>])) + <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## add a column with value 1 at the left side of x </span></span><br><span class="line">x_with_constant = np.insert(x,<span class="number">0</span>,<span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">x_t = np.transpose(x_with_constant)</span><br><span class="line"></span><br><span class="line">beta = np.linalg.inv(x_t.dot(x_with_constant)).dot(x_t).dot(y)</span><br><span class="line"><span class="comment">## beta is array([3., 1., 2.])</span></span><br><span class="line">y_pred =  np.array([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>]]).dot(beta)</span><br><span class="line"></span><br><span class="line">y_pred</span><br><span class="line"><span class="comment">## array([16.])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="comment"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class="line">y = np.dot(X, np.array([<span class="number">1</span>, <span class="number">2</span>])) + <span class="number">3</span></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">reg.score(X, y)</span><br><span class="line"><span class="comment">###  1.0</span></span><br><span class="line">eg.coef_</span><br><span class="line"><span class="comment">## array([1., 2.])</span></span><br><span class="line">reg.intercept_ </span><br><span class="line"><span class="comment">### 3.0000...</span></span><br><span class="line">reg.predict(np.array([[<span class="number">3</span>, <span class="number">5</span>]]))</span><br><span class="line"><span class="comment">## array([16.])</span></span><br></pre></td></tr></table></figure><p>Comparing to the function in sckit-learn, both of them can do the correct prediction. And other performance can be consider later…</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf" target="_blank" rel="noopener">http://mezeylab.cb.bscb.cornell.edu/labmembers/documents/supplement%205%20-%20multiple%20regression.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> linear model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AIC and BIC</title>
      <link href="/2019/07/29/aic-bic/"/>
      <url>/2019/07/29/aic-bic/</url>
      
        <content type="html"><![CDATA[<p>Introduction 2 model estimation AIC and BIC.</p><a id="more"></a><h2 id="AIC"><a href="#AIC" class="headerlink" title="AIC"></a>AIC</h2><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>As you know, AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form </p><div style="text-align:center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;AIC=-2ln(\hat{L}) + 2k" class="lozad"></div><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;BIC=-2ln(\hat{L}) + ln(n)k" class="lozad"></p><p>where $\hat{L}$ is the max value of likelihood function for the model, n is the number of data points(observations), and k is number of estimated parameter.</p><p>where $\hat{L}$ is the max value of likelihood function for the model. And likelihood function is how likely particular values of statistical parameters are for a given set of observations and is defined as </p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;L(\theta | x) = f(x | \theta)" class="lozad"></p><p>where $f(x | \theta)$ is joint probability density function, $k$ is the number of estimated parameter in our model.</p><p>For AIC information criterion, it contains information of goodness which is represented in likelihood. And AIC also include the penalty part of number of estimated parameter as a increasing function. As we know increasing the number of parameters in the model almost always improves the goodness of the fit, the penalty discourages overfitting.</p><p>AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC.</p><p>AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n. BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n, of choosing too small a model.</p><p>So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Timeseries</title>
      <link href="/2019/07/25/timeseries/"/>
      <url>/2019/07/25/timeseries/</url>
      
        <content type="html"><![CDATA[<hr><p>Introduction of ARIMA and SARIMA model </p><a id="more"></a><h2 id="ARIMA-autoregreesive-integrated-moving-average"><a href="#ARIMA-autoregreesive-integrated-moving-average" class="headerlink" title="ARIMA-autoregreesive integrated moving average"></a>ARIMA-autoregreesive integrated moving average</h2><p>Before we select the model, we need to test the stationarity of the data. A time series is stationary if it has constant mean and variance, and covariance is independent of time. The test I used is Dickey-Fuller test, the null hypothesis is that a unit root exists. If there is a unit root exists, then p &gt; \alpha, we say the process is not stationary at \alpha significant level. </p><p>In the common time series model, we have autoregressive (AR) model–AR(p), moving average (MA) model–MA(q), autoregressive–moving-average (ARMA)–ARMA(p,q) and Autoregressive integrated moving average (ARIMA)–ARIMA(p,d,q) where p,d,q stands for seasonality, trend, and noise in data.  </p><ul><li>AR: Auto-Regressive (p): AR terms are just lags of dependent variable. For example lets say p is 3, we will use x(t-1), x(t-2) and x(t-3) to predict x(t)</li><li>I: Integrated (d): These are the number of non-seasonal differences. For example, in our case we take the first order difference. So we pass that variable and put d=1</li><li>MA: Moving Averages (q): MA terms are lagged forecast errors in prediction equation.</li></ul><p>The way I use find the suitable model is to try to look at autocorrelation and partial-autocorrelation which will give us a first idea to select the range of parameters. Then based on the AIC(Akaike information criterion) which can define the goodness of a model, to test the parameter. </p><p>In the test, we found that the original time series is not stationary. Then we can try the first order difference and try to look at weekly data to get more smooth trend.</p><p>To find the better parameter, we can use AIC or BIC. But note that, when models are compared using these values, it is important that all models have the same orders of differencing. If a model has a order of differencing (d) of a model, then the data is changed on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable.</p><h2 id="Seasonal-autoregressive-integrated-moving-average-model-SARIMA"><a href="#Seasonal-autoregressive-integrated-moving-average-model-SARIMA" class="headerlink" title="Seasonal autoregressive integrated moving average model(SARIMA)"></a>Seasonal autoregressive integrated moving average model(SARIMA)</h2><p>SARIMA (p,d,q) * (P,D,Q,S) where (p,d,q) is same as ARIMA (These three parameters account for seasonality, trend, and noise in data), and P is the seasonal autoregressive component, D is the seasonal difference, Q is the seasonal moving average component, S is the length of the season.</p><p>The general formula for SARIMA with seasonal period as 12(months per year) is:</p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;\phi(B)\Phi(B^{12})y_t = \theta_0+\theta(B)\Theta(B^{12})\epsilon_t" class="lozad"></p> <p>where </p><p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; AR: \phi(B) = 1 - \phi_1B - ... - \phi_pB^p" class="lozad"></p> <p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space; MA: \theta(B) = 1 + \theta_1B + ... + \theta_qB^q" class="lozad"></p> <p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;Seasonal AR: \Phi(B^S) = 1 - \Phi_1B^S - ... - \Phi_PB^PS" class="lozad"></p> <p align="center"><img data-src="https://latex.codecogs.com/svg.latex?\Large&space;Seasonal MA: \Theta(B^S) = 1 + \Theta_1B^S + ... + \Theta_QB^QS" class="lozad"></p> <p>Since we already find there is a seasonal part in our data, then seasonal differencing will be used. The data we used later is the linear combination of the few Keywords and Heat Index between 2017/01/01 to 2019/07/13.</p><p>To compare each model with different parameter, I used log-likelihood to find out the better parameter in ARIMA. Smaller log-likelihood, AIC or BIC means this model is better.</p><p>Some comparison:</p><table><thead><tr><th>ARIMA model</th><th>mean square error</th></tr></thead><tbody><tr><td>ARIMA(3,0,1)(2,1,2,12)</td><td>275</td></tr><tr><td>ARIMA(3,1,2)(2,1,2,12)</td><td>279</td></tr><tr><td>ARIMA(3,0,2)(0,1,2,12)</td><td>260</td></tr><tr><td>ARIMA(3,0,2)(1,1,2,12)</td><td>266</td></tr><tr><td>ARIMA(3,0,1)(1,1,2,12)</td><td>276</td></tr><tr><td>ARIMA(2,0,1)(0,1,2,12)</td><td>268</td></tr><tr><td>ARIMA(2,1,1)(0,1,2,12)</td><td>271</td></tr><tr><td>ARIMA(2,1,1)(0,1,2,12)</td><td>271</td></tr><tr><td>ARIMA(3,1,1)(0,1,2,12)</td><td>270</td></tr><tr><td>…</td><td></td></tr></tbody></table><p>Then ARIMA(3,0,2)(0,1,2,12) may be better here.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Classification Application</title>
      <link href="/2019/07/25/classification_app/"/>
      <url>/2019/07/25/classification_app/</url>
      
        <content type="html"><![CDATA[<p>Introduction of several Classification packages in machine learning(Python3).</p><a id="more"></a> <p>Before we use many ML algorithm, we sometimes need to preprocess the data</p><p>Import all package</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># Import train_test_split function</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics <span class="comment">#Import scikit-learn metrics module for accuracy calculation</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import model package</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier <span class="comment"># Import Decision Tree Classifier</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br></pre></td></tr></table></figure><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>Function to encode the data type, change all object type features into dummy variables </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> df.columns:</span><br><span class="line">        <span class="keyword">if</span> df[column].dtype == type(object):</span><br><span class="line">            le = preprocessing.LabelEncoder()</span><br><span class="line">            df[column] = le.fit_transform(df[column])</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><p>Function to normalise the data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalised</span><span class="params">(df)</span>:</span></span><br><span class="line">    min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">    x_scaled = min_max_scaler.fit_transform(df.values)</span><br><span class="line">    df_new = pd.DataFrame(x_scaled, columns= df.columns)</span><br><span class="line">    <span class="keyword">return</span> df_new</span><br></pre></td></tr></table></figure><p>Or we can use the package in scikit learn of <a href="https://scikit-learn.org/stable/data_transforms.html" target="_blank" rel="noopener">data transformation</a>. Where fit() is to calculate properties such like mean, min, max and so on for training process.  transform() is to apply normalise, reduce dimension and regularization base on fitted data. fit_transform combined these two methods.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">sc.fit_tranform(x_train)</span><br><span class="line">sc.tranform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">mms = MinMaxScaler()</span><br><span class="line">mms.fit_tranform(x_train)</span><br><span class="line">mms.tranform(x_test)</span><br></pre></td></tr></table></figure><p>Using PCA as an example of reducing dimension</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components = <span class="number">3</span>)</span><br><span class="line">pca.fit_transdorm()</span><br></pre></td></tr></table></figure><p>Spliting the data frame into train and test to varify the goodness of model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a fixed randome_state num will have same train and test set </span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(dfhouse, df_encode[[<span class="string">"prod_id"</span>]], test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p>Logistic Regression Classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression_classifier</span><span class="params">(train_x, train_y)</span>:</span></span><br><span class="line">    model = LogisticRegression(penalty=<span class="string">'l2'</span>, solver = <span class="string">"lbfgs"</span>, multi_class=<span class="string">'auto'</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">lgr_model = logistic_regression_classifier(train_x, train_y)</span><br><span class="line">pred_y = lgr_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>Naive Bayes Classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_bayes_classifier</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    model = GaussianNB()</span><br><span class="line">    model.fit(x, y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">nb_model = naive_bayes_classifier(train_x, train_y)</span><br><span class="line">pred_y = nb_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>KNN K-Nearest Neighbors Classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn_classifier</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    model = KNeighborsClassifier()</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model </span><br><span class="line"></span><br><span class="line">knn_model = knn_classifier(train_x, train_y)</span><br><span class="line">pred_y = knn_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>Random Forest Classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest_classifier</span><span class="params">(train_x, train_y)</span>:</span></span><br><span class="line">    model = RandomForestClassifier(max_depth=<span class="number">2</span>, random_state=<span class="number">0</span>, )</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">rf_model = random_forest_classifier(train_x, train_y)</span><br><span class="line">rf_model.feature_importances_  <span class="comment"># show feature importance for each feature</span></span><br><span class="line">model = SelectFromModel(rf_model, prefit=<span class="literal">True</span>) <span class="comment"># select no zero coefficient features</span></span><br><span class="line">x_new = model.transform(x)</span><br><span class="line">x_new.shape()</span><br><span class="line"></span><br><span class="line">train_x_rf = train_x.iloc[:, my_list] <span class="comment"># can mutually define my_list = [] to select important feature</span></span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>Decision Tree Classifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision_tree_classifier</span><span class="params">(train_x, train_y)</span>:</span></span><br><span class="line">    model = tree.DecisionTreeClassifier()</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">dt_model = decision_tree_classifier(train_x, train_y)</span><br><span class="line">pred_y = dt_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>GBDT-Gradient Boosting Decision Tree Classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_boosting_classifier</span><span class="params">(train_x, train_y)</span>:</span></span><br><span class="line">    model = GradientBoostingClassifier(n_estimators=<span class="number">200</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">gbdt_model = gradient_boosting_classifier(train_x, train_y)</span><br><span class="line">pred_y = gbdt_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure><p>SVM-Support Vaector Machine Classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_classifier</span><span class="params">(train_x, train_y)</span>:</span></span><br><span class="line">    model = SVC(kernel=<span class="string">'rbf'</span>, probability=<span class="literal">True</span>, gamma = <span class="string">"auto"</span>)</span><br><span class="line">    model.fit(train_x, train_y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">svm_model = svm_classifier(train_x, train_y)</span><br><span class="line">pred_y = svm_model.predict(test_x)</span><br><span class="line">metrics.accuracy_score(test_y, pred_y)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Find important feature</title>
      <link href="/2019/07/25/feature_test/"/>
      <url>/2019/07/25/feature_test/</url>
      
        <content type="html"><![CDATA[<p>Introduction of feature engineering. Include some method such like p-value and F-test in Pyhton.</p><a id="more"></a><h2 id="Select-feature"><a href="#Select-feature" class="headerlink" title="Select feature"></a>Select feature</h2><h3 id="Reason-to-do-feature-selection"><a href="#Reason-to-do-feature-selection" class="headerlink" title="Reason to do feature selection"></a>Reason to do feature selection</h3><ol><li>Because a large number of features may cost long training time</li><li>Increasing number of features may increase the risk of overfitting<br>It can also help us to reduce the dimension of our dataset without loss main information.<h3 id="Methods-of-feature-selection"><a href="#Methods-of-feature-selection" class="headerlink" title="Methods of feature selection"></a>Methods of feature selection</h3><h4 id="P-value"><a href="#P-value" class="headerlink" title="P-value"></a>P-value</h4>P-value is the probability of Null hypothesis is true in statisitc model. Normally we select p-value = 0.05 as an significant level.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="comment">#Adding constant column of ones, mandatory for sm.OLS model</span></span><br><span class="line">x = pd.DataFrame() <span class="comment"># feature df</span></span><br><span class="line">y = pd.DataFrame() <span class="comment"># target df</span></span><br><span class="line">x_1 = sm.add_constant(x) <span class="comment"># add a constant column to df x</span></span><br><span class="line"><span class="comment">#Fitting sm.OLS model</span></span><br><span class="line">model = sm.OLS(np.array(y), np.array(x_1)).fit()</span><br><span class="line"><span class="comment"># Which prints out the p-value of each features in this model as an array </span></span><br><span class="line">model.pvalues</span><br></pre></td></tr></table></figure></li></ol><p>Write a function to select feature based on p-value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x is feature df</span></span><br><span class="line"><span class="comment"># y is target df</span></span><br><span class="line"><span class="comment"># sl is significant level</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backwardElimination</span><span class="params">(x, y, sl)</span>:</span></span><br><span class="line">    cols = list(x.columns)</span><br><span class="line">    pmax = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> (len(cols) &gt; <span class="number">0</span>):</span><br><span class="line">        p= []                                                                                   </span><br><span class="line">        X_1 = x[cols]</span><br><span class="line">        X_1 = sm.add_constant(X_1)</span><br><span class="line">        model = sm.OLS(y, X_1).fit()</span><br><span class="line">        p = pd.Series(model.pvalues.values[<span class="number">1</span>:], index = cols)      </span><br><span class="line">        pmax = max(p)</span><br><span class="line">        <span class="comment"># .idxmax returns the index of maximum</span></span><br><span class="line">        feature_with_p_max = p.idxmax()</span><br><span class="line">        <span class="keyword">if</span>(pmax &gt; sl):</span><br><span class="line">            cols.remove(feature_with_p_max)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    selected_features_BE = cols</span><br><span class="line">    print(selected_features_BE)</span><br><span class="line"></span><br><span class="line">backwardElimination(x, y, <span class="number">0.05</span>)</span><br></pre></td></tr></table></figure><h4 id="F-test"><a href="#F-test" class="headerlink" title="F-test"></a>F-test</h4><p>F-test is a statistical test to find whether there is a significant difference between two model. Least square error is calculated for each model and compared.</p><p>Here introduced the <a href="https://scikit-learn.org/stable/" target="_blank" rel="noopener">skitlearn package</a>, we will use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html" target="_blank" rel="noopener">F-test</a> to find first K best features:</p><p>For continues data type</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> sklearn.feature_selection</span><br><span class="line">sklearn.feature_selection.f_regression(x, y) <span class="comment"># where x is feature df(n_sample * n_features), y is target df (n_samples)</span></span><br><span class="line"><span class="comment"># output is set of F-score and p-value for each F-score</span></span><br></pre></td></tr></table></figure><p>For classification data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.f_classif(x, y) <span class="comment"># same with f_regression</span></span><br><span class="line">sklearn.feature_selection.chi2(x, y) <span class="comment"># if x is sparse, then only use chi2 can still keep it sparsity.</span></span><br></pre></td></tr></table></figure><p>F-score is good for linear relation</p><h4 id="Mutual-infomation"><a href="#Mutual-infomation" class="headerlink" title="Mutual infomation"></a>Mutual infomation</h4><p>If x and y is independent, MI is 0. And if x has y relation or x is a function of y then MI is 0.<br>More detail in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" target="_blank" rel="noopener">sklearn Mutual information</a><br>Which is good for non-linear relation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x is feature df, y is target df</span></span><br><span class="line"><span class="comment"># For discrete_features. If ‘auto’, it is assigned to False for dense X and to True for sparse X</span></span><br><span class="line"><span class="comment"># n_neighbor higher values reduce variance of the estimation</span></span><br><span class="line">sklearn.feature_selection.mututal_info_regression(x, y, discrete_features=’auto’, n_neighbors=<span class="number">3</span>, copy=<span class="literal">True</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">sklearn.feature_selection.mututal_info_classif(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output is estimated MI between each feature and target</span></span><br></pre></td></tr></table></figure><h4 id="Variance-threshold"><a href="#Variance-threshold" class="headerlink" title="Variance threshold"></a>Variance threshold</h4><p>Which is only care about feature itself: if it not vary a lot, then it has poor predictive power.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Read data</title>
      <link href="/2019/07/25/read_data/"/>
      <url>/2019/07/25/read_data/</url>
      
        <content type="html"><![CDATA[<p>Include a method to read(scratch) data from website and transfer them into dataframe. </p><a id="more"></a><h3 id="csv-file"><a href="#csv-file" class="headerlink" title="csv file"></a>csv file</h3><p>Using Pandas package  <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" target="_blank" rel="noopener">read_csv</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">"data.csv"</span>, encoding = <span class="string">"utf-8"</span>) <span class="comment"># check encoding type such like "utf-16"</span></span><br></pre></td></tr></table></figure><h3 id="data-from-website"><a href="#data-from-website" class="headerlink" title="data from website"></a>data from website</h3><p>Reading data about house price and house feature from a website as an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soup_to_df</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="comment">#define column name</span></span><br><span class="line">    dfcols = [<span class="string">'outcode'</span>, <span class="string">'last_published_date'</span>,<span class="string">'latitude'</span>, <span class="string">'longitude'</span>, <span class="string">'post_town'</span>, <span class="string">'num_bathrooms'</span>, <span class="string">'num_bedrooms'</span>, <span class="string">'num_floors'</span>, </span><br><span class="line">              <span class="string">'num_recepts'</span>, <span class="string">'property_type'</span>, <span class="string">'street_name'</span>, <span class="string">"price"</span>]</span><br><span class="line">    df_xml = pd.DataFrame(columns=dfcols)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> s.find_all(<span class="string">"listing"</span>):</span><br><span class="line">        outcode =  node.find(<span class="string">'outcode'</span>).get_text()</span><br><span class="line">        last_published_date = node.find(<span class="string">'last_published_date'</span>).get_text()</span><br><span class="line">        latitude = node.find(<span class="string">'latitude'</span>).get_text()</span><br><span class="line">        longitude = node.find(<span class="string">'longitude'</span>).get_text()</span><br><span class="line">        post_town = node.find(<span class="string">'post_town'</span>).get_text()</span><br><span class="line">        num_bathrooms = node.find(<span class="string">'num_bathrooms'</span>).get_text()</span><br><span class="line">        num_bedrooms = node.find(<span class="string">'num_bedrooms'</span>).get_text()</span><br><span class="line">        num_floors = node.find(<span class="string">'num_floors'</span>).get_text()</span><br><span class="line">        num_recepts = node.find(<span class="string">'num_recepts'</span>).get_text()</span><br><span class="line">        property_type = node.find(<span class="string">'property_type'</span>).get_text()</span><br><span class="line">        street_name = node.find(<span class="string">'street_name'</span>).get_text()</span><br><span class="line">        price = node.find(<span class="string">'price'</span>).get_text()</span><br><span class="line"></span><br><span class="line">        df_xml = df_xml.append(pd.Series([outcode, float(latitude), float(longitude), post_town, last_published_date, int(num_bathrooms),</span><br><span class="line">                                          int(num_bedrooms), int(num_floors), int(num_recepts), property_type, street_name, int(price)], index=dfcols),ignore_index=<span class="literal">True</span>)   </span><br><span class="line">    <span class="keyword">return</span> df_xml</span><br><span class="line"></span><br><span class="line">dfcols = [<span class="string">'outcode'</span>, <span class="string">'latitude'</span>, <span class="string">'longitude'</span>, <span class="string">'post_town'</span>, <span class="string">'last_published_date'</span>, <span class="string">'num_bathrooms'</span>, <span class="string">'num_bedrooms'</span>, <span class="string">'num_floors'</span>, </span><br><span class="line">              <span class="string">'num_recepts'</span>, <span class="string">'property_type'</span>, <span class="string">'street_name'</span>, <span class="string">"price"</span>]</span><br><span class="line"></span><br><span class="line">df_all = pd.DataFrame(columns=dfcols)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">58</span>): <span class="comment">#page range</span></span><br><span class="line"><span class="comment"># Ctrl+Shift+I in that website to find the url for which we would like to scrape. </span></span><br><span class="line">    baseurl = <span class="string">f"https://api.zoopla.co.uk/api/v1/property_listings?api_key=9zpbeza9n858g3u2g633u3rb&amp;county=Somerset&amp;country=England&amp;listing_status=sale&amp;include_sold=1&amp;page_number=<span class="subst">&#123;i&#125;</span>&amp;page_size=100"</span></span><br><span class="line">    page = requests.get(baseurl)</span><br><span class="line">    soup = BeautifulSoup(page.content)</span><br><span class="line">    <span class="comment">#save data from one page into a dataframe</span></span><br><span class="line">    df = soup_to_df(soup)</span><br><span class="line">    <span class="comment"># combine all data from all pages into one dataframe</span></span><br><span class="line">    df_all = pd.concat([df_all, df], axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
